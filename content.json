{"meta":{"title":"莫坤东的博客","subtitle":null,"description":null,"author":"小白","url":"https://mokundong.cn"},"pages":[{"title":"","date":"2019-10-25T09:40:40.186Z","updated":"2019-02-21T11:34:20.574Z","comments":true,"path":"googlea31bfe60691413ce.html","permalink":"https://mokundong.cn/googlea31bfe60691413ce.html","excerpt":"","text":"google-site-verification: googlea31bfe60691413ce.html"},{"title":"about","date":"2018-12-12T14:14:36.000Z","updated":"2019-10-25T08:38:51.697Z","comments":true,"path":"about/index.html","permalink":"https://mokundong.cn/about/index.html","excerpt":"","text":"简历项目","keywords":"关于"},{"title":"categories","date":"2019-01-13T04:38:07.000Z","updated":"2019-02-21T11:34:20.956Z","comments":true,"path":"categories/index.html","permalink":"https://mokundong.cn/categories/index.html","excerpt":"","text":""},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2019-10-15T15:10:33.000Z","comments":false,"path":"client/index.html","permalink":"https://mokundong.cn/client/index.html","excerpt":"","text":"直接下载 or 扫码下载：","keywords":"Android客户端"},{"title":"bangumi","date":"2019-02-10T13:32:48.000Z","updated":"2019-10-15T15:10:33.000Z","comments":false,"path":"bangumi/index.html","permalink":"https://mokundong.cn/bangumi/index.html","excerpt":"","text":"","keywords":null},{"title":"lab","date":"2019-01-05T13:47:59.000Z","updated":"2019-10-15T15:10:33.000Z","comments":false,"path":"lab/index.html","permalink":"https://mokundong.cn/lab/index.html","excerpt":"","text":"sakura主题balabala","keywords":"Lab实验室"},{"title":"links","date":"2018-12-19T15:11:06.000Z","updated":"2019-10-15T15:10:33.000Z","comments":true,"path":"links/index.html","permalink":"https://mokundong.cn/links/index.html","excerpt":"","text":"","keywords":"友人帐"},{"title":"donate","date":"2018-12-20T15:13:05.000Z","updated":"2019-10-15T15:10:33.000Z","comments":false,"path":"donate/index.html","permalink":"https://mokundong.cn/donate/index.html","excerpt":"","text":"","keywords":"谢谢饲主了喵~"},{"title":"music","date":"2018-12-20T15:14:28.000Z","updated":"2019-10-15T15:10:33.000Z","comments":false,"path":"music/index.html","permalink":"https://mokundong.cn/music/index.html","excerpt":"","text":"","keywords":"喜欢的音乐"},{"title":"comment","date":"2018-12-20T15:13:48.000Z","updated":"2019-10-15T15:10:33.000Z","comments":true,"path":"comment/index.html","permalink":"https://mokundong.cn/comment/index.html","excerpt":"","text":"念两句诗 叙别梦、扬州一觉。 【宋代】吴文英《夜游宫·人去西楼雁杳》","keywords":"留言板"},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2019-10-15T15:10:33.000Z","comments":true,"path":"tags/index.html","permalink":"https://mokundong.cn/tags/index.html","excerpt":"","text":""},{"title":"rss","date":"2018-12-20T15:09:03.000Z","updated":"2019-10-15T15:10:33.000Z","comments":true,"path":"rss/index.html","permalink":"https://mokundong.cn/rss/index.html","excerpt":"","text":""},{"title":"theme-sakura","date":"2019-01-04T14:53:25.000Z","updated":"2019-10-15T15:10:33.000Z","comments":false,"path":"theme-sakura/index.html","permalink":"https://mokundong.cn/theme-sakura/index.html","excerpt":"","text":"Hexo主题Sakura修改自WordPress主题Sakura，感谢原作者Mashiro","keywords":"Hexo 主题 Sakura 🌸"},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2019-10-15T15:10:33.000Z","comments":false,"path":"video/index.html","permalink":"https://mokundong.cn/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '放送时间: 2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 番组计划 这里将是永远的回忆 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"B站"}],"posts":[{"title":"Qlearning-tutorial-part0","slug":"Qlearning-tensorflow-tutorial-part0","date":"2019-10-25T13:56:48.000Z","updated":"2019-10-25T14:37:25.783Z","comments":true,"path":"Qlearning-tensorflow-tutorial-part0/","link":"","permalink":"https://mokundong.cn/Qlearning-tensorflow-tutorial-part0/","excerpt":"","text":"&gt; 本文翻译自 Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks， 作者是 Arthur Juliani，原文链接。 We’ll be learning how to solve the OpenAI FrozenLake environment. Our version is a little less photo-realistic. 在本强化学习系列教程中，我们将要探讨一系列称为Q-Learning的增强学习算法。与接下来的三章(Part1-3)介绍的基于策略(policy-base)的增强学习算法有所不同。 我们将从实现一个简单查找表算法开始，然后展示如何使用Tensorflow来实现等效的神经网络。考虑到上述安排，我们需要回顾基础知识，所以将这一篇视为本系列的第0部分。希望通过这个系列的教程，我们在理解Q-learning之后，能够结合policy gradient和Q-learning方法构建更好的增强学习网络。（如果你对策略网络更感兴趣或者你已经有一些Q-learning的经验，那么你可以从这里开始阅读） &nbsp; 策略梯度算法(policy gradient)试着学习某个函数，该函数可以直接把状态(state)映射为动作(action)的概率分布。Q-learning和策略梯度算法不一样，它试着学习在每个状态下对应的值，并且依赖该状态执行某一个动作。虽然两种方法最终都允许我们给定情况下采取特定的行动，但是实现该目的的方法是不相同的。你也许已经听说深度Q-learning可以玩atari游戏，我们将要在这里讨论和实现这些更复杂和强大的Q-learning算法。 Tabular Approaches for Tabular Environment(表环境下的表方法) ​ The rules of the FrozenLake environment 在本教程中，我们将要尝试使用OpenAI gym解决FrozenLake问题。OpenAI gym提供了一个简单的环境，使初学者可以在他们提供一系列游戏中尝试他们的方法。比如FrozenLake，该游戏环境包括一个4*4的网络格子，每个格子可以是起始块，目标块、冻结块或者危险块。我们的目标是让Agent学习从开始块到目标快而不陷入危险块。在任意时刻，Agent能够任意选择上下左右方向，偶尔也会跳到并非Agent选择的方向。因此，不是每一步都能达到目标快，但是学习避免危险达到目标仍然是可行的。每一步除了达到目标奖励为1以外，其他奖励都是0。因此，需要学习一种长期奖励的算法，这就是Q-Learning学习的要诀。 在最简单的实现中，Q-Learning是环境中每种状态（行）和动作（列）的表。在表格的每个单元格中，我们学习一个值，该值表示在给定状态下执行给定操作的效果如何。对于FrozenLake环境，我们有16个可能的状态（每个块一个）和4个可能的动作（四个运动方向），从而为我们提供了一个16x4的Q值表。我们首先将表格初始化为统一的（全零），然后观察各种动作所获得的回报，然后相应地更新表格。我们使用称为Bellman方程对Q表进行更新，该方程表示给定动作的预期长期奖励等于当前动作的即时奖励与在采取的最佳未来动作的预期奖励相结合。这样，在估算如何为将来的操作更新表时，我们将重用自己的Q表！在方程式中，规则如下所示： $$\\mathrm{Eq} 1 . \\mathrm{Q}(\\mathrm{s}, \\mathrm{a})=\\mathrm{r}+\\mathrm{\\gamma}\\left(\\max \\left(\\mathrm{Q}\\left(\\mathrm{s}^{\\prime}, \\mathrm{a}^{\\prime}\\right)\\right)\\right.$$ 这个公式的描述了一个给定状态s和行动a下的Q值等于当即获得的回报r加上一个折现因子γ乘以能够最大化的在下一状态s’采取时能获得的最大长期回报的动作a’对应的长期回报。折现因子y允许我们决定相对于当前就可以获得的回报，未来的可能回报的相对重要性。通过这种方式，Q表会慢慢开始获得更准确的任一给定状态下，采取任意动作所对应的期望未来回报值。以下是Python版的对于Q表版本的冰湖环境解决方案的完整实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import gymimport numpy as npenv = gym.make('FrozenLake-v0')# Implement Q-Table learning algorithm# 实现Q表学习算法# Initialize table with all zeros# 初始化Q表为全0值Q = np.zeros([env.observation_space.n,env.action_space.n])# Set learning parameters# 设置学习参数lr = .8y = .95num_episodes = 2000# create lists to contain total rewards and steps per episode# 创建列表以包含每个episode的总回报与总步数jList = []rList = []for i in range(num_episodes): # Reset environment and get first new observation # 初始化环境并获得第一个观察 s = env.reset() rAll = 0 d = False j = 0 # The Q-Table learning algorithm # Q表学习算法 while j &lt; 99: j+=1 # Choose an action by greedily (with noise) picking from Q table # 基于Q表贪婪地选择一个最优行动（有噪音干扰） a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1))) # Get new state and reward from environment # 从环境中获得回报和新的状态信息 s1,r,d,_ = env.step(a) #Update Q-Table with new knowledge # 用新的知识更新Q表 Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a]) rAll += r s = s1 if d == True: break #jList.append(j) rList.append(rAll)print(\"Score over time: \" + str(sum(rList)/num_episodes))print(\"Final Q-Table Values\")print(Q) (感谢Praneet D找到了该实现方法对应的最优的超参数） ## Q-Learning with Neural Networks(基于神经网络的Q-Learning) 现在你可能认为：表格方法挺好的，但是它不能规模化（scale），不是吗？因为对一个简单的网格世界建立一个164的表是很容易的，但是在任何一个现在的游戏或真实世界环境中都有无数可能的状态。对于大多数有趣的问题，表格都无法发挥出作用。因此，我们需要一些代替性的方案来描述我们的状态，并生成对应动作的Q值：这也就是***神经网络（Neural Network，简称NN）****可以大展身手的地方。NN可以作为一个动作估计器（function approximator），我们能够输入任意多的可能状态，因为所有状态都可以被编码成一个个向量，并把它们和各自对应的Q值进行对应（map）。 在冰湖例子中，我们将使用一个一层的NN，它接受以one-hot形式编码的状态向量（116），并输出一个含有4个Q值的向量，每个分量对应一个动作的长期期望回报。这样一个简单的NN就像一个强化版的Q表，而网络的权重就发挥着曾经的Q表中的各个单元格的作用。关键的不同时我们可以方便简易地扩充Tensorflow网络，包括加新的层，激活函数以及不同的输出类型，而这些都是一个一般的表格无法做到的。于是，更新的方法也发生了一点变化。相比于之前直接更新表格，我们现在将使用***逆传播（backpropagation）和**损失函数（loss function）来完成更新。我们的损失函数采取平方和损失的形式，即加总当前预测的Q值与目标值间的差值的平方，并以梯度形式在网络中传播。这种情况下，所选行动的目标Q值依然采用上面提到的贝尔曼方程中的计算方法。 $$\\mathrm{Eq2} .\\mathrm{Loss}=\\sum(\\mathrm{Q}-{target}-\\mathrm{Q})^2$$ 以下是Tensorflow实现简单的Q-Network的完整代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# Q-Network Learning# Q网络学习import gymimport numpy as npimport randomimport tensorflow as tfimport matplotlib.pyplot as plt%matplotlib inline# Load the environment# 加载环境env = gym.make('FrozenLake-v0')# Q网络方法# Implementing the network itself# 实现网络tf.reset_default_graph()# These lines establish the feed-forward part of the network used to choose actions# 下面的几行代码建立了网络的前馈部分，它将用于选择行动inputs1 = tf.placeholder(shape=[1,16],dtype=tf.float32)W = tf.Variable(tf.random_uniform([16,4],0,0.01))Qout = tf.matmul(inputs1,W)predict = tf.argmax(Qout,1)# Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.# 下面的几行代码可以获得预测Q值与目标Q值间差值的平方和加总的损失。nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32)loss = tf.reduce_sum(tf.square(nextQ - Qout))trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)updateModel = trainer.minimize(loss)# Training the network# 训练网络init = tf.initialize_all_variables()# Set learning parameters# 设置学习参数y = .99e = 0.1num_episodes = 2000#create lists to contain total rewards and steps per episode# 创建列表以包含每个episode对应的总回报与总步数。jList = []rList = []with tf.Session() as sess: sess.run(init) for i in range(num_episodes): # Reset environment and get first new observation # 初始化环境并获得第一个观察 s = env.reset() rAll = 0 d = False j = 0 # The Q-Network # Q网络 while j &lt; 99: j+=1 #Choose an action by greedily (with e chance of random action) from the Q-network # 基于Q网络的输出结果，贪婪地选择一个行动（有一定的概率选择随机行动） a,allQ = sess.run([predict,Qout],feed_dict=&#123;inputs1:np.identity(16)[s:s+1]&#125;) if np.random.rand(1) &lt; e: a[0] = env.action_space.sample() # Get new state and reward from environment # 从环境中获得回报以及新的状态信息 s1,r,d,_ = env.step(a[0]) # Obtain the Q' values by feeding the new state through our network # 通过将新的状态向量输入到网络中获得Q值。 Q1 = sess.run(Qout,feed_dict=&#123;inputs1:np.identity(16)[s1:s1+1]&#125;) # Obtain maxQ' and set our target value for chosen action. # 获得最大的Q值，并为所选行为设定目标值 maxQ1 = np.max(Q1) targetQ = allQ targetQ[0,a[0]] = r + y*maxQ1 # Train our network using target and predicted Q values # 用目标和预测的Q值训练网络 _,W1 = sess.run([updateModel,W],feed_dict=&#123;inputs1:np.identity(16[s:s+1],nextQ:targetQ&#125;) rAll += r s = s1 if d == True: # Reduce chance of random action as we train the model. # 随着训练的进行，主键减少选择随机行为的概率 e = 1./((i/50) + 10) break jList.append(j) rList.append(rAll)print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")# Percent of succesful episodes: 0.352%# 成功的episode比例：0.352% Some statistics on network performance We can see that the network beings to consistly reach the goal around the 750 episode mark. 1plt.plot(rList) 1plt.plot(jList) 虽然网络学会了解决冰湖问题，但是结果表明它似乎比如Q表方法那么高效。即虽然神经网络在Q-Learning问题上提供了更高的灵活性，但它也牺牲了一定的稳定性。还有很多可能的对我们的简单Q网络进行扩展的办法，这些扩展可以让NN获得更好的性能并实现更稳定的学习过程。两种需要提到的特别技巧分别是\\经验重放（Experience Replay）和冰冻目标网络（Freezing Target Networks）**。这些改进方式或者其它的一些技巧都是让DQN可以玩Atari游戏的关键所在，并且我们也将在后面探索这些相关知识。对于有关Q-Learning的更多理论，可以看Tambet Matiisen的这篇博文，希望本教程可以帮助对实现简单Q-Learning算法的人们一些帮助！","categories":[{"name":"强化学习","slug":"强化学习","permalink":"https://mokundong.cn/categories/强化学习/"},{"name":"tutorial","slug":"强化学习/tutorial","permalink":"https://mokundong.cn/categories/强化学习/tutorial/"}],"tags":[],"keywords":[{"name":"强化学习","slug":"强化学习","permalink":"https://mokundong.cn/categories/强化学习/"},{"name":"tutorial","slug":"强化学习/tutorial","permalink":"https://mokundong.cn/categories/强化学习/tutorial/"}]},{"title":"share-python-books","slug":"share-python-books","date":"2019-10-25T09:51:46.000Z","updated":"2019-10-25T14:05:45.916Z","comments":true,"path":"share-python-books/","link":"","permalink":"https://mokundong.cn/share-python-books/","excerpt":"","text":"python books 序号 链接 提取码 1 python cookbook 中文 kswl 2 python3 程序开发指南 kvqo 3 python3 廖雪峰指南 8ngw 4 python高性能编程 eaud 5 流畅的python vok4 说明 入门建议首选廖雪峰的python指南或者参考Python3 菜鸟教程；","categories":[{"name":"资料分享","slug":"资料分享","permalink":"https://mokundong.cn/categories/资料分享/"},{"name":"pythonbook","slug":"资料分享/pythonbook","permalink":"https://mokundong.cn/categories/资料分享/pythonbook/"}],"tags":[],"keywords":[{"name":"资料分享","slug":"资料分享","permalink":"https://mokundong.cn/categories/资料分享/"},{"name":"pythonbook","slug":"资料分享/pythonbook","permalink":"https://mokundong.cn/categories/资料分享/pythonbook/"}]},{"title":"flask部署深度学习模型","slug":"python-flask-deeplearing-service","date":"2019-10-25T08:44:39.000Z","updated":"2019-10-25T09:47:29.903Z","comments":true,"path":"python-flask-deeplearing-service/","link":"","permalink":"https://mokundong.cn/python-flask-deeplearing-service/","excerpt":"","text":"作为著名Python web框架之一的Flask，具有简单轻量、灵活、扩展丰富且上手难度低的特点，因此成为了机器学习和深度学习模型上线跑定时任务，提供API的首选框架。众所周知，Flask默认不支持非阻塞IO的，当请求A还未完成时候，请求B需要等待请求A完成后才能被处理，所以效率非常低。但是线上任务通常需要异步、高并发等需求，本文总结一些在日常使用过程中所常用的技巧。 一、前沿 异步和多线程有什么区别？其实，异步是目的，而多线程是实现这个目的的方法。异步是说，A发起一个操作后（一般都是比较耗时的操作，如果不耗时的操作就没有必要异步了），可以继续自顾自的处理它自己的事儿，不用干等着这个耗时操作返回。实现异步可以采用多线程技术或则交给另外的进程来处理,详解常见这里。 二、实现方法 Flask启动自带方法 采用gunicorn部署 1、Flask中自带方法实现 run.py #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2018-12-01 16:37 # @Author : mokundong from flask import Flask import socket from time import sleep myhost = socket.gethostbyname(socket.gethostname()) app = Flask(__name__) @app.route('/job1') def some_long_task1(): print(\"Task #1 started!\") sleep(10) print(\"Task #1 is done!\") @app.route('/job2') def some_long_task2(arg1, arg2): print(\"Task #2 started with args: %s %s!\" % (arg1, arg2)) sleep(5) print(\"Task #2 is done!\") if __name__ == '__main__': app.run(host=myhost,port=5000,threaded=True) app.run(host=xxx,port=xx,threaded=True)中threaded开启后则不需要等队列。 2、gunicorn部署 Gunicorn 是一个高效的Python WSGI Server,通常用它来运行 wsgi application 或者 wsgi framework(如Django,Paster,Flask),地位相当于Java中的Tomcat。gunicorn 会启动一组 worker进程，所有worker进程公用一组listener，在每个worker中为每个listener建立一个wsgi server。每当有HTTP链接到来时，wsgi server创建一个协程来处理该链接，协程处理该链接的时候，先初始化WSGI环境，然后调用用户提供的app对象去处理HTTP请求。关于gunicorn的详细说明，可以参考这里。 使用命令行启动gunicorn有两种方式获取配置项，一种是在命令行配置，一种是在配置文件中获取。 run.py #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2018-12-01 17:00 # @Author : mokundong from flask import Flask from time import sleep app = Flask(__name__) @app.route('/job1') def some_long_task1(): print(\"Task #1 started!\") sleep(10) print(\"Task #1 is done!\") @app.route('/job2') def some_long_task2(arg1, arg2): print(\"Task #2 started with args: %s %s!\" % (arg1, arg2)) sleep(5) print(\"Task #2 is done!\") if __name__ == '__main__': app.run() 命令行配置gunicorn --workers=4 --bind=127.0.0.1:8000 run:app 更多配置见官网 配置文件获取配置 gunicorn_config.py #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2018-12-01 17:10 # @Author : mokundong import os import socket import multiprocessing import gevent.monkey gevent.monkey.patch_all() myhost = socket.gethostbyname(socket.gethostname()) debug = False loglevel = 'info' hosts = get_host_ip() bind = hosts+\":5000\" timeout = 30 #超时 pidfile = \"log/gunicorn.pid\" accesslog = \"log/access.log\" errorlog = \"log/debug.log\" daemon = True #意味着开启后台运行，默认为False workers = 4 # 启动的进程数 threads = 2 #指定每个进程开启的线程数 worker_class = 'gevent' #默认为sync模式，也可使用gevent模式。 x_forwarded_for_header = 'X-FORWARDED-FOR' 启动命令如下 gunicorn -c gunicorn_config.py run:app 三、补充1、关于线程的补充 在工作中我还遇到一种情况，当一个请求过来后，我需要两种回应，一个是及时返回app运行结果，第二个响应是保存数据到日志或者数据库。往往我们在写数据的过程中会花销一定的时间，导致结果返回会有所延迟，因此我们需要用两个线程处理这两个任务，那么我们如下处理。 run.py #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2018-12-01 17:20 # @Author : mokundong from flask import Flask,request from time import sleep from concurrent.futures import ThreadPoolExecutor executor = ThreadPoolExecutor(2) app = Flask(__name__) @app.route('/job') def run_jobs(): executor.submit(some_long_task1) executor.submit(some_long_task2, 'hello', 123) return 'Two jobs was launched in background!' def some_long_task1(): print(\"Task #1 started!\") sleep(10) print(\"Task #1 is done!\") def some_long_task2(arg1, arg2): print(\"Task #2 started with args: %s %s!\" % (arg1, arg2)) sleep(5) print(\"Task #2 is done!\") if __name__ == '__main__': app.run() 2、关于获取IP的补充 上述代码中通过获取hostname，然后再通过hostname反查处机器的IP。这个方法是不推荐的。因为很多的机器没有规范这个hostname的设置。另外就是有些服务器会在 /etc/hosts 中添加本机的hostname的地址，这个做法也不是不可以，但是如果设置成了 127.0.0.1，那么获取出来的IP就都是这个地址了。这里给出一种优雅的方式获取IP，利用 UDP 协议来实现的，生成一个UDP包，把自己的 IP 放如到 UDP 协议头中，然后从UDP包中获取本机的IP。 #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2018-12-01 17:30 # @Author : mokundong # 可以封装成函数，方便 Python 的程序调用 import socket def get_host_ip(): try: s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) s.connect(('8.8.8.8', 80)) ip = s.getsockname()[0] finally: s.close() return ip 总结当然推荐使用gunicorn部署多线程，Flask自带的，emmmm，测试玩儿玩儿吧。在写作过程中才发现自己知识漏洞不是一般多，共勉！","categories":[{"name":"python","slug":"python","permalink":"https://mokundong.cn/categories/python/"},{"name":"flask","slug":"python/flask","permalink":"https://mokundong.cn/categories/python/flask/"}],"tags":[],"keywords":[{"name":"python","slug":"python","permalink":"https://mokundong.cn/categories/python/"},{"name":"flask","slug":"python/flask","permalink":"https://mokundong.cn/categories/python/flask/"}]},{"title":"Summary-2018-12","slug":"chat-Summary-2018","date":"2018-12-30T09:44:00.000Z","updated":"2019-10-25T14:06:07.358Z","comments":true,"path":"chat-Summary-2018/","link":"","permalink":"https://mokundong.cn/chat-Summary-2018/","excerpt":"","text":"2018年可谓人生一大转折点，这年中结束了长达二十年的求学生涯摇身一变就要两肩扛砖，为一口吃喝而奔波。前半年忙论文忙答辩除了忙碌还是忙碌，不过心情还是极好的，毕竟对面长达三年身心折磨的结束还是很期待的；忘记与学校分手的第几天起，喜欢一个人…啊，呸，从七月的第一个日出那天起入职培训过渡到项目，渐渐开始融入万千互联网农民工的大军中。 一、源 其实昨天晚上我就在想我是不是也要跟个俗，写一个年度总结，纠结了很久最终还是想着这重要的一年还是要留下点什么吧。我想写写工作吧，学校留下的，大概也只有师兄姐弟妹之间的一些东西了。 二、关于工作 当初还是有那么几个工作机会摆在我面面前的，对于最后的选择我还是满意的，一来我比较喜欢现在组内的工作氛围，大家都比较照顾我；二是能够做我喜欢做的事情，也能学到很多东西；最后也能离家很近，有很多熟悉的朋友都还在。 学习 刚入职场的新人，学习总是重要的环节，不管是专业知识，工程能力还是项目沟通能力等等。 1、刚进公司，我是在数据科学团队的。这边主要做一些研究性质的事情，这段时间我的重心主要是一些理论学习。开始我没有想到这边主要做的是nlp一类的项目，而我在这一块算是比较欠缺的（毕竟我是一个学物理的，找工作的时候又看得比较杂，广而不精），所以很多时候的在看论文、看代码、逛技术博客，那段特别感谢赵xx哥和朱x兄带我。后来DS团队合并到搜索推荐组了，赵哥也走了。 2、说实话刚到搜索这边的时候，我还是比较担心的，毕竟这边工程上的事情很多，而我在这一块可能需要补的东西更多，好在老大家林哥比较照顾我，让我还是继续做之前的东西，也总是替我背锅。但研究项目终究需要工程化应用才能产生价值，这段时间非常感谢凯哥，总是很耐心的指导我一些工程上的流程和细节，总体过得也算顺利。后来朱兄和凯哥也走了。 3、那段时间走了好多人，在学校习惯了身边总是那些熟悉的面孔，突然很不适应这种状况，但后来慢慢的也习惯了。 收获 既然有学习，当然就有收获的啦。这半年我做的一个主要项目是一个与文本分类相关的事情。项目从研究、选模型到落地应用持续了好几个月，与加州大学某分校有合作。 1、在这个项目过程中，理论上学习到了比较多的内容，从CNN到Transform的特征提取方式，从word embeding到BERT框架的预训练框架均有所进一步认识。 2、 3、项目方沟通过程中，加强了与业务方的沟通能力，了解业务对模型，对工程的影响。 4、最终，模型成功应用于重要环节，在效果上相比加州大学提高了一倍，这一点我还是很满意的。 不足 自身的不足还是很明显的，特别是以后接触更多的项目，欠下的东西会越发的凸显出来。 1、Java忘记得差不多了，很多工程上的东西还没学。 2、很多基础算法的推导也渐渐模糊了。 3、身上的肉，嗯，好像还是那么多。 三、写在最后 这一年，准确说是半年大概就是这么个情况，有所获有欠缺，来年继续努力，查漏补缺。目前行业的发展有这么一个段子：“2019年可能会是过去十年里最差的一年，但却是未来十年里最好的一年”。革命尚未结束，同志仍需努力，诸君共勉！！ 2018.12.31 莫坤东","categories":[{"name":"日记杂谈","slug":"日记杂谈","permalink":"https://mokundong.cn/categories/日记杂谈/"}],"tags":[],"keywords":[{"name":"日记杂谈","slug":"日记杂谈","permalink":"https://mokundong.cn/categories/日记杂谈/"}]},{"title":"keras-test","slug":"python-keras-test","date":"2018-12-12T14:16:01.000Z","updated":"2019-10-25T13:53:00.285Z","comments":true,"path":"python-keras-test/","link":"","permalink":"https://mokundong.cn/python-keras-test/","excerpt":"","text":"12import tensorflow as tfimport tensorflow.keras as keras","categories":[{"name":"python","slug":"python","permalink":"https://mokundong.cn/categories/python/"},{"name":"keras","slug":"python/keras","permalink":"https://mokundong.cn/categories/python/keras/"}],"tags":[],"keywords":[{"name":"python","slug":"python","permalink":"https://mokundong.cn/categories/python/"},{"name":"keras","slug":"python/keras","permalink":"https://mokundong.cn/categories/python/keras/"}]}]}