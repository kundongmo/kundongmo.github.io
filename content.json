{"meta":{"title":"莫坤东的博客","subtitle":null,"description":null,"author":"小白","url":"https://mokundong.cn"},"pages":[{"title":"","date":"2019-10-31T12:50:43.996Z","updated":"2019-02-21T11:34:20.574Z","comments":true,"path":"googlea31bfe60691413ce.html","permalink":"https://mokundong.cn/googlea31bfe60691413ce.html","excerpt":"","text":"google-site-verification: googlea31bfe60691413ce.html"},{"title":"about","date":"2018-12-12T14:14:36.000Z","updated":"2019-10-25T08:38:51.697Z","comments":true,"path":"about/index.html","permalink":"https://mokundong.cn/about/index.html","excerpt":"","text":"简历项目","keywords":"关于"},{"title":"bangumi","date":"2019-02-10T13:32:48.000Z","updated":"2019-10-15T15:10:33.000Z","comments":false,"path":"bangumi/index.html","permalink":"https://mokundong.cn/bangumi/index.html","excerpt":"","text":"","keywords":null},{"title":"client","date":"2018-12-20T15:13:35.000Z","updated":"2019-10-15T15:10:33.000Z","comments":false,"path":"client/index.html","permalink":"https://mokundong.cn/client/index.html","excerpt":"","text":"直接下载 or 扫码下载：","keywords":"Android客户端"},{"title":"categories","date":"2019-01-13T04:38:07.000Z","updated":"2019-02-21T11:34:20.956Z","comments":true,"path":"categories/index.html","permalink":"https://mokundong.cn/categories/index.html","excerpt":"","text":""},{"title":"comment","date":"2018-12-20T15:13:48.000Z","updated":"2019-10-15T15:10:33.000Z","comments":true,"path":"comment/index.html","permalink":"https://mokundong.cn/comment/index.html","excerpt":"","text":"念两句诗 叙别梦、扬州一觉。 【宋代】吴文英《夜游宫·人去西楼雁杳》","keywords":"留言板"},{"title":"donate","date":"2018-12-20T15:13:05.000Z","updated":"2019-10-15T15:10:33.000Z","comments":false,"path":"donate/index.html","permalink":"https://mokundong.cn/donate/index.html","excerpt":"","text":"","keywords":"谢谢饲主了喵~"},{"title":"lab","date":"2019-01-05T13:47:59.000Z","updated":"2019-10-15T15:10:33.000Z","comments":false,"path":"lab/index.html","permalink":"https://mokundong.cn/lab/index.html","excerpt":"","text":"sakura主题balabala","keywords":"Lab实验室"},{"title":"links","date":"2018-12-19T15:11:06.000Z","updated":"2019-10-15T15:10:33.000Z","comments":true,"path":"links/index.html","permalink":"https://mokundong.cn/links/index.html","excerpt":"","text":"","keywords":"友人帐"},{"title":"music","date":"2018-12-20T15:14:28.000Z","updated":"2019-10-15T15:10:33.000Z","comments":false,"path":"music/index.html","permalink":"https://mokundong.cn/music/index.html","excerpt":"","text":"","keywords":"喜欢的音乐"},{"title":"rss","date":"2018-12-20T15:09:03.000Z","updated":"2019-10-15T15:10:33.000Z","comments":true,"path":"rss/index.html","permalink":"https://mokundong.cn/rss/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2019-10-15T15:10:33.000Z","comments":true,"path":"tags/index.html","permalink":"https://mokundong.cn/tags/index.html","excerpt":"","text":""},{"title":"theme-sakura","date":"2019-01-04T14:53:25.000Z","updated":"2019-10-15T15:10:33.000Z","comments":false,"path":"theme-sakura/index.html","permalink":"https://mokundong.cn/theme-sakura/index.html","excerpt":"","text":"Hexo主题Sakura修改自WordPress主题Sakura，感谢原作者Mashiro","keywords":"Hexo 主题 Sakura 🌸"},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2019-10-15T15:10:33.000Z","comments":false,"path":"video/index.html","permalink":"https://mokundong.cn/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '放送时间: 2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 番组计划 这里将是永远的回忆 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"B站"}],"posts":[{"title":"特征加权之TFIWF","slug":"nlp-basic-keyword-tfiwf","date":"2019-10-31T13:01:33.000Z","updated":"2019-11-01T05:39:37.605Z","comments":true,"path":"nlp-basic-keyword-tfiwf/","link":"","permalink":"https://mokundong.cn/nlp-basic-keyword-tfiwf/","excerpt":"","text":"1、背景在上篇特征加权之TFIDF 中，我们提到了IDF的固有缺点，即$IDF$ 的简单结构并不能使提取的关键词，十分有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能。尤其是在同类语料库中，这一方法有很大弊端，往往一些同类文本的关键词被掩盖。例如：语料库 $D$ 中教育类文章偏多，而文本 $j $是一篇属于教育类的文章，那么教育类相关的词语的$ IDF $值将会偏小，使提取文本关键词的召回率更低。因此我们在此提出词语逆频率方式计算加权算法 $TF-IWF$ (Term Frequency-Inverse Word Frequency)。 2、TF-IWF此处的$TF$与$TF-IDF$中意义一样，表示词频：$$tf_{ij} = \\frac{n_{i,j}}{\\sum_{k}n_{k,j}}$$上式中分子$ n_{i,j}$ 表示词语$t_i$在文本$j$中的频数，分母$\\sum_{k}n_{k,j}$表示文档$j$中所有词汇量总和，即是说： $$TF_w = \\frac{给定词w在当前文章出现的次数}{当前文章中的总词量}$$不同之处在于$IWF$部分，定义为：$$iwf_{i} = log\\frac{\\sum_{i=1}^m nt_{i}}{nt_{i}}$$上式中分子 $\\sum_{i=1}^m nt_{i}$ 表示语料库中所有词语的频数之和，分母 $nt_{i}$ 表示词语 $t_{i}$ 在语料库中的总频数，即：$$IWF_{i}=\\frac{语料库中所有词语的频数}{给定词w在语料库中出现的频数和}$$ 因此，$TF-IWF$ 定义为：$$TF-IWF_{i,j}\\rightarrow tf_{i,j} \\times iwf_{i}=\\frac{n_{i,j}}{\\sum_{k}n_{k,j}}\\times log\\frac{\\sum_{i=1}^m nt_{i}}{nt_{i}}$$ 3、代码实现4、总结这种加权方法降低了语料库中同类型文本对词语权重的影响，更加精确地表达了这个词语在待 查文档中的重要程度。在传统方法 $TF-IDF$ 所求的权值一般很小接近 $0$，精确度也不是很高，公式 $TF-IWF$ 的计算结果刚好能解决最后权值过小的问题。","categories":[{"name":"nlp","slug":"nlp","permalink":"https://mokundong.cn/categories/nlp/"},{"name":"主题抽取","slug":"nlp/主题抽取","permalink":"https://mokundong.cn/categories/nlp/主题抽取/"}],"tags":[],"keywords":[{"name":"nlp","slug":"nlp","permalink":"https://mokundong.cn/categories/nlp/"},{"name":"主题抽取","slug":"nlp/主题抽取","permalink":"https://mokundong.cn/categories/nlp/主题抽取/"}]},{"title":"特征加权之TFIDF","slug":"nlp-basic-keyword-tfidf","date":"2019-10-31T03:44:27.000Z","updated":"2019-11-01T03:10:50.800Z","comments":true,"path":"nlp-basic-keyword-tfidf/","link":"","permalink":"https://mokundong.cn/nlp-basic-keyword-tfidf/","excerpt":"","text":"1、背景TF-IDF是信息检索和文本挖掘中常用的特征加权技术，同样常用于文本主题提取和分词加权等场景。 TF-IDF是一种完全基于统计的方法，其核心思想是假设字词的重要性与其在某篇文章中出现的比例成正比，与其在其他文章中出现的比例成反比。 2、TF-IDF怎么理解呢？某个词在一篇文章中反复出现有两种情况，即这个词是关键词或者常用词(可理解为停用词)。要确定这个词的最终属性则需要考虑这个词在其他文章中出现的频率，若其他文章出现频率高则可以判断为常用词，出现频率低则可以确定为该词所属文章的主题词。 2.1、TFTF (Trem Frequency) 表示词频，即一个词在文章中出现的次数。实际应用中不同长度的文章中指定词出现的次数可能与文档长度有正相关，因此我们需要对词频进行归一化处理，通常用次数除以文章的总词数：$$tf_{ij} = \\frac{n_{i,j}}{\\sum_{k}n_{k,j}}$$公式中分子$ n_{i,j}$ 表示词语$t_i$在文本$j$中的频数，分母$\\sum_{k}n_{k,j}$表示文档$j$中所有词汇量总和。 即是说：$$TF_w = \\frac{给定词w在当前文章出现的次数}{当前文章中的总词量}$$$TF$在应用中一般是在分词的时候在线计算。 2.2、IDFIDF(Inverse Document Frequency)表示逆文档率，定义为文件总数除以包含给定词出现的文档数。$$idf_{i} = log\\frac{\\left|D\\right|}{\\left|\\left\\{j:t_i\\in d_j\\right\\}\\right|}$$公式中$\\left|D\\right|$表示语料库$d$的文档总数，分母$\\left|\\left\\{j:t_i\\in d_j\\right\\}\\right|$表示语料库$d$中包含文档$j$中词语$t_i$的文档数，实际应用中为了避免分母为$0$，因此分母一般采用$\\left|1 + \\left\\{j:t_i\\in d_j\\right\\}\\right|$。 即是说：$$IDF=log \\frac{语料库文档总数}{包含词w的文档数}$$$IDF$在应用中一般是采用业务相关语料离线计算。 2.3、TF-IDF$TF-IDF$定义为：$$TF-IDF_{i,j}\\rightarrow tf_{i,j} \\times idf_{i}=\\frac{n_{i,j}}{\\sum_{k}n_{k,j}}\\times log \\frac{\\left|D\\right|}{\\left|1+\\left\\{j:t_{i}\\in{d_{j}}\\right\\}\\right|}$$以上就是TF-IDF算法的全部理论。 2.4、实现目前很多分词框架和机器学习算法包均集成了TF-IDF算法，以下做一个简要的罗列： 2.4.1、结巴分词(Python)123import jieba.analysestr = \"自然语言是人类智慧的结晶，自然语言处理是人工智能中最为困难的问题之一，而对自然语言处理的研究也是充满魅力和挑战的。\"jieba.analyse.extract_tags(s, topK=10, withWeight=True, allowPOS=()) 越是重要的词语所给予的权重就越大。 2.4.2、Scikit-learnScikit-learn提供了自己训练的模型的接口 12345678910111213141516171819202122232425#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/10/30 09:48# @Author : mokundongfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.feature_extraction.text import TfidfTransformerx_train = ['这是 第一 篇 文章 ，', '这 篇 文章 是 第二 篇 文章 。', '这是 第三 篇 文章 。' ]x_test = ['这是 第几 篇 文章 ？']CV = CountVectorizer(max_features=10)transformer = TfidfTransformer()tf_idf = transformer.fit_transform(CV.fit_transform(x_train))x_train_weight = tf_idf.toarray()tf_idf = transformer.transform(CV.transform(x_test))x_test_weight = tf_idf.toarray()print(x_test_weight)'''output:[[0.61335554 0. 0. 0. 0.78980693]]''' 2.4.3、笔者在项目中，一般采取的方法是：a、首先用相关的文本训练IDF值保存文件； b、在项目中首先初始化文件到内存，保存为字典便于快速查询； c、新文档读入后实时计算TF值并查询相关IDF值计算出TF-IDF值。 2.5 总结本质上$IDF$是一种试图抑制噪声的加权，单纯的以为文本频率小的单词就越重要，文本频率大的单词就越无用。这对于大部分文本信息，并不是完全正确的。$IDF$ 的简单结构并不能使提取的关键词，十分有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能。尤其是在同类语料库中，这一方法有很大弊端，往往一些同类文本的关键词被掩盖。例如：语料库 $D$ 中教育类文章偏多，而文本 $j $是一篇属于教育类的文章，那么教育类相关的词语的$ IDF $值将会偏小，使提取文本关键词的召回率更低。因此才会有词语逆频率方式计算加权算法 $TF-IWF$ (Term Frequency-Inverse Word Frequency)，关于$TF-IWF$将会在下篇文章讲解。 并且，笔者在工作实践中发现针对于短句应用$TF-IDF$提取关键词时，由于短句中每个词的$DF$值往往是相同的，鉴于上述$IDF$天然的弱点，此算法应用于短句分析也显得不可靠，针对短句这种情况，笔者的思路是结合词性和黑白名单以及搜索点击数据和业务打tag等，需要比较综合的方式来解决短句问题。 参考文献 http://dx.doi.org/10.12677/csa.2013.31012","categories":[{"name":"nlp","slug":"nlp","permalink":"https://mokundong.cn/categories/nlp/"},{"name":"主题抽取","slug":"nlp/主题抽取","permalink":"https://mokundong.cn/categories/nlp/主题抽取/"}],"tags":[],"keywords":[{"name":"nlp","slug":"nlp","permalink":"https://mokundong.cn/categories/nlp/"},{"name":"主题抽取","slug":"nlp/主题抽取","permalink":"https://mokundong.cn/categories/nlp/主题抽取/"}]},{"title":"Qlearning-tutorial-part0","slug":"Qlearning-tensorflow-tutorial-part0","date":"2019-10-25T13:56:48.000Z","updated":"2019-11-01T01:34:25.487Z","comments":true,"path":"Qlearning-tensorflow-tutorial-part0/","link":"","permalink":"https://mokundong.cn/Qlearning-tensorflow-tutorial-part0/","excerpt":"","text":"&gt; 本文翻译自 Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks， 作者是 Arthur Juliani，原文链接。 We’ll be learning how to solve the OpenAI FrozenLake environment. Our version is a little less photo-realistic. 在本强化学习系列教程中，我们将要探讨一系列称为Q-Learning的增强学习算法。与接下来的三章(Part1-3)介绍的基于策略(policy-base)的增强学习算法有所不同。 我们将从实现一个简单查找表算法开始，然后展示如何使用Tensorflow来实现等效的神经网络。考虑到上述安排，我们需要回顾基础知识，所以将这一篇视为本系列的第0部分。希望通过这个系列的教程，我们在理解Q-learning之后，能够结合policy gradient和Q-learning方法构建更好的增强学习网络。（如果你对策略网络更感兴趣或者你已经有一些Q-learning的经验，那么你可以从这里开始阅读） &nbsp; 策略梯度算法(policy gradient)试着学习某个函数，该函数可以直接把状态(state)映射为动作(action)的概率分布。Q-learning和策略梯度算法不一样，它试着学习在每个状态下对应的值，并且依赖该状态执行某一个动作。虽然两种方法最终都允许我们给定情况下采取特定的行动，但是实现该目的的方法是不相同的。你也许已经听说深度Q-learning可以玩atari游戏，我们将要在这里讨论和实现这些更复杂和强大的Q-learning算法。 Tabular Approaches for Tabular Environment(表环境下的表方法) ​ The rules of the FrozenLake environment 在本教程中，我们将要尝试使用OpenAI gym解决FrozenLake问题。OpenAI gym提供了一个简单的环境，使初学者可以在他们提供一系列游戏中尝试他们的方法。比如FrozenLake，该游戏环境包括一个4*4的网络格子，每个格子可以是起始块，目标块、冻结块或者危险块。我们的目标是让Agent学习从开始块到目标快而不陷入危险块。在任意时刻，Agent能够任意选择上下左右方向，偶尔也会跳到并非Agent选择的方向。因此，不是每一步都能达到目标快，但是学习避免危险达到目标仍然是可行的。每一步除了达到目标奖励为1以外，其他奖励都是0。因此，需要学习一种长期奖励的算法，这就是Q-Learning学习的要诀。 在最简单的实现中，Q-Learning是环境中每种状态（行）和动作（列）的表。在表格的每个单元格中，我们学习一个值，该值表示在给定状态下执行给定操作的效果如何。对于FrozenLake环境，我们有16个可能的状态（每个块一个）和4个可能的动作（四个运动方向），从而为我们提供了一个16x4的Q值表。我们首先将表格初始化为统一的（全零），然后观察各种动作所获得的回报，然后相应地更新表格。我们使用称为Bellman方程对Q表进行更新，该方程表示给定动作的预期长期奖励等于当前动作的即时奖励与在采取的最佳未来动作的预期奖励相结合。这样，在估算如何为将来的操作更新表时，我们将重用自己的Q表！在方程式中，规则如下所示： $$\\mathrm{Eq} 1 . \\mathrm{Q}(\\mathrm{s}, \\mathrm{a})=\\mathrm{r}+\\mathrm{\\gamma}\\left(\\max \\left(\\mathrm{Q}\\left(\\mathrm{s}^{\\prime}, \\mathrm{a}^{\\prime}\\right)\\right)\\right.$$ 这个公式的描述了一个给定状态s和行动a下的Q值等于当即获得的回报r加上一个折现因子γ乘以能够最大化的在下一状态s’采取时能获得的最大长期回报的动作a’对应的长期回报。折现因子y允许我们决定相对于当前就可以获得的回报，未来的可能回报的相对重要性。通过这种方式，Q表会慢慢开始获得更准确的任一给定状态下，采取任意动作所对应的期望未来回报值。以下是Python版的对于Q表版本的冰湖环境解决方案的完整实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import gymimport numpy as npenv = gym.make('FrozenLake-v0')# Implement Q-Table learning algorithm# 实现Q表学习算法# Initialize table with all zeros# 初始化Q表为全0值Q = np.zeros([env.observation_space.n,env.action_space.n])# Set learning parameters# 设置学习参数lr = .8y = .95num_episodes = 2000# create lists to contain total rewards and steps per episode# 创建列表以包含每个episode的总回报与总步数jList = []rList = []for i in range(num_episodes): # Reset environment and get first new observation # 初始化环境并获得第一个观察 s = env.reset() rAll = 0 d = False j = 0 # The Q-Table learning algorithm # Q表学习算法 while j &lt; 99: j+=1 # Choose an action by greedily (with noise) picking from Q table # 基于Q表贪婪地选择一个最优行动（有噪音干扰） a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1))) # Get new state and reward from environment # 从环境中获得回报和新的状态信息 s1,r,d,_ = env.step(a) #Update Q-Table with new knowledge # 用新的知识更新Q表 Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a]) rAll += r s = s1 if d == True: break #jList.append(j) rList.append(rAll)print(\"Score over time: \" + str(sum(rList)/num_episodes))print(\"Final Q-Table Values\")print(Q) (感谢Praneet D找到了该实现方法对应的最优的超参数） ## Q-Learning with Neural Networks(基于神经网络的Q-Learning) 现在你可能认为：表格方法挺好的，但是它不能规模化（scale），不是吗？因为对一个简单的网格世界建立一个164的表是很容易的，但是在任何一个现在的游戏或真实世界环境中都有无数可能的状态。对于大多数有趣的问题，表格都无法发挥出作用。因此，我们需要一些代替性的方案来描述我们的状态，并生成对应动作的Q值：这也就是***神经网络（Neural Network，简称NN）****可以大展身手的地方。NN可以作为一个动作估计器（function approximator），我们能够输入任意多的可能状态，因为所有状态都可以被编码成一个个向量，并把它们和各自对应的Q值进行对应（map）。 在冰湖例子中，我们将使用一个一层的NN，它接受以one-hot形式编码的状态向量（116），并输出一个含有4个Q值的向量，每个分量对应一个动作的长期期望回报。这样一个简单的NN就像一个强化版的Q表，而网络的权重就发挥着曾经的Q表中的各个单元格的作用。关键的不同时我们可以方便简易地扩充Tensorflow网络，包括加新的层，激活函数以及不同的输出类型，而这些都是一个一般的表格无法做到的。于是，更新的方法也发生了一点变化。相比于之前直接更新表格，我们现在将使用***逆传播（backpropagation）和**损失函数（loss function）来完成更新。我们的损失函数采取平方和损失的形式，即加总当前预测的Q值与目标值间的差值的平方，并以梯度形式在网络中传播。这种情况下，所选行动的目标Q值依然采用上面提到的贝尔曼方程中的计算方法。 $$\\mathrm{Eq2} .\\mathrm{Loss}=\\sum(\\mathrm{Q}-{target}-\\mathrm{Q})^2$$ 以下是Tensorflow实现简单的Q-Network的完整代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# Q-Network Learning# Q网络学习import gymimport numpy as npimport randomimport tensorflow as tfimport matplotlib.pyplot as plt%matplotlib inline# Load the environment# 加载环境env = gym.make('FrozenLake-v0')# Q网络方法# Implementing the network itself# 实现网络tf.reset_default_graph()# These lines establish the feed-forward part of the network used to choose actions# 下面的几行代码建立了网络的前馈部分，它将用于选择行动inputs1 = tf.placeholder(shape=[1,16],dtype=tf.float32)W = tf.Variable(tf.random_uniform([16,4],0,0.01))Qout = tf.matmul(inputs1,W)predict = tf.argmax(Qout,1)# Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.# 下面的几行代码可以获得预测Q值与目标Q值间差值的平方和加总的损失。nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32)loss = tf.reduce_sum(tf.square(nextQ - Qout))trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)updateModel = trainer.minimize(loss)# Training the network# 训练网络init = tf.initialize_all_variables()# Set learning parameters# 设置学习参数y = .99e = 0.1num_episodes = 2000#create lists to contain total rewards and steps per episode# 创建列表以包含每个episode对应的总回报与总步数。jList = []rList = []with tf.Session() as sess: sess.run(init) for i in range(num_episodes): # Reset environment and get first new observation # 初始化环境并获得第一个观察 s = env.reset() rAll = 0 d = False j = 0 # The Q-Network # Q网络 while j &lt; 99: j+=1 #Choose an action by greedily (with e chance of random action) from the Q-network # 基于Q网络的输出结果，贪婪地选择一个行动（有一定的概率选择随机行动） a,allQ = sess.run([predict,Qout],feed_dict=&#123;inputs1:np.identity(16)[s:s+1]&#125;) if np.random.rand(1) &lt; e: a[0] = env.action_space.sample() # Get new state and reward from environment # 从环境中获得回报以及新的状态信息 s1,r,d,_ = env.step(a[0]) # Obtain the Q' values by feeding the new state through our network # 通过将新的状态向量输入到网络中获得Q值。 Q1 = sess.run(Qout,feed_dict=&#123;inputs1:np.identity(16)[s1:s1+1]&#125;) # Obtain maxQ' and set our target value for chosen action. # 获得最大的Q值，并为所选行为设定目标值 maxQ1 = np.max(Q1) targetQ = allQ targetQ[0,a[0]] = r + y*maxQ1 # Train our network using target and predicted Q values # 用目标和预测的Q值训练网络 _,W1 = sess.run([updateModel,W],feed_dict=&#123;inputs1:np.identity(16[s:s+1],nextQ:targetQ&#125;) rAll += r s = s1 if d == True: # Reduce chance of random action as we train the model. # 随着训练的进行，主键减少选择随机行为的概率 e = 1./((i/50) + 10) break jList.append(j) rList.append(rAll)print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")# Percent of succesful episodes: 0.352%# 成功的episode比例：0.352% Some statistics on network performance We can see that the network beings to consistly reach the goal around the 750 episode mark. 1plt.plot(rList) 1plt.plot(jList) 虽然网络学会了解决冰湖问题，但是结果表明它似乎比如Q表方法那么高效。即虽然神经网络在Q-Learning问题上提供了更高的灵活性，但它也牺牲了一定的稳定性。还有很多可能的对我们的简单Q网络进行扩展的办法，这些扩展可以让NN获得更好的性能并实现更稳定的学习过程。两种需要提到的特别技巧分别是\\经验重放（Experience Replay）和冰冻目标网络（Freezing Target Networks）**。这些改进方式或者其它的一些技巧都是让DQN可以玩Atari游戏的关键所在，并且我们也将在后面探索这些相关知识。对于有关Q-Learning的更多理论，可以看Tambet Matiisen的这篇博文，希望本教程可以帮助对实现简单Q-Learning算法的人们一些帮助！","categories":[{"name":"强化学习","slug":"强化学习","permalink":"https://mokundong.cn/categories/强化学习/"},{"name":"tutorial","slug":"强化学习/tutorial","permalink":"https://mokundong.cn/categories/强化学习/tutorial/"}],"tags":[],"keywords":[{"name":"强化学习","slug":"强化学习","permalink":"https://mokundong.cn/categories/强化学习/"},{"name":"tutorial","slug":"强化学习/tutorial","permalink":"https://mokundong.cn/categories/强化学习/tutorial/"}]},{"title":"share-python-books","slug":"share-python-books","date":"2019-10-25T09:51:46.000Z","updated":"2019-11-01T01:34:34.503Z","comments":true,"path":"share-python-books/","link":"","permalink":"https://mokundong.cn/share-python-books/","excerpt":"","text":"python books 序号 链接 提取码 1 python cookbook 中文 kswl 2 python3 程序开发指南 kvqo 3 python3 廖雪峰指南 8ngw 4 python高性能编程 eaud 5 流畅的python vok4 说明 入门建议首选廖雪峰的python指南或者参考Python3 菜鸟教程；","categories":[{"name":"资料分享","slug":"资料分享","permalink":"https://mokundong.cn/categories/资料分享/"},{"name":"pythonbook","slug":"资料分享/pythonbook","permalink":"https://mokundong.cn/categories/资料分享/pythonbook/"}],"tags":[],"keywords":[{"name":"资料分享","slug":"资料分享","permalink":"https://mokundong.cn/categories/资料分享/"},{"name":"pythonbook","slug":"资料分享/pythonbook","permalink":"https://mokundong.cn/categories/资料分享/pythonbook/"}]},{"title":"flask部署深度学习模型","slug":"python-flask-deeplearing-service","date":"2019-10-25T08:44:39.000Z","updated":"2019-10-25T09:47:29.903Z","comments":true,"path":"python-flask-deeplearing-service/","link":"","permalink":"https://mokundong.cn/python-flask-deeplearing-service/","excerpt":"","text":"作为著名Python web框架之一的Flask，具有简单轻量、灵活、扩展丰富且上手难度低的特点，因此成为了机器学习和深度学习模型上线跑定时任务，提供API的首选框架。众所周知，Flask默认不支持非阻塞IO的，当请求A还未完成时候，请求B需要等待请求A完成后才能被处理，所以效率非常低。但是线上任务通常需要异步、高并发等需求，本文总结一些在日常使用过程中所常用的技巧。 一、前沿 异步和多线程有什么区别？其实，异步是目的，而多线程是实现这个目的的方法。异步是说，A发起一个操作后（一般都是比较耗时的操作，如果不耗时的操作就没有必要异步了），可以继续自顾自的处理它自己的事儿，不用干等着这个耗时操作返回。实现异步可以采用多线程技术或则交给另外的进程来处理,详解常见这里。 二、实现方法 Flask启动自带方法 采用gunicorn部署 1、Flask中自带方法实现 run.py #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2018-12-01 16:37 # @Author : mokundong from flask import Flask import socket from time import sleep myhost = socket.gethostbyname(socket.gethostname()) app = Flask(__name__) @app.route('/job1') def some_long_task1(): print(\"Task #1 started!\") sleep(10) print(\"Task #1 is done!\") @app.route('/job2') def some_long_task2(arg1, arg2): print(\"Task #2 started with args: %s %s!\" % (arg1, arg2)) sleep(5) print(\"Task #2 is done!\") if __name__ == '__main__': app.run(host=myhost,port=5000,threaded=True) app.run(host=xxx,port=xx,threaded=True)中threaded开启后则不需要等队列。 2、gunicorn部署 Gunicorn 是一个高效的Python WSGI Server,通常用它来运行 wsgi application 或者 wsgi framework(如Django,Paster,Flask),地位相当于Java中的Tomcat。gunicorn 会启动一组 worker进程，所有worker进程公用一组listener，在每个worker中为每个listener建立一个wsgi server。每当有HTTP链接到来时，wsgi server创建一个协程来处理该链接，协程处理该链接的时候，先初始化WSGI环境，然后调用用户提供的app对象去处理HTTP请求。关于gunicorn的详细说明，可以参考这里。 使用命令行启动gunicorn有两种方式获取配置项，一种是在命令行配置，一种是在配置文件中获取。 run.py #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2018-12-01 17:00 # @Author : mokundong from flask import Flask from time import sleep app = Flask(__name__) @app.route('/job1') def some_long_task1(): print(\"Task #1 started!\") sleep(10) print(\"Task #1 is done!\") @app.route('/job2') def some_long_task2(arg1, arg2): print(\"Task #2 started with args: %s %s!\" % (arg1, arg2)) sleep(5) print(\"Task #2 is done!\") if __name__ == '__main__': app.run() 命令行配置gunicorn --workers=4 --bind=127.0.0.1:8000 run:app 更多配置见官网 配置文件获取配置 gunicorn_config.py #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2018-12-01 17:10 # @Author : mokundong import os import socket import multiprocessing import gevent.monkey gevent.monkey.patch_all() myhost = socket.gethostbyname(socket.gethostname()) debug = False loglevel = 'info' hosts = get_host_ip() bind = hosts+\":5000\" timeout = 30 #超时 pidfile = \"log/gunicorn.pid\" accesslog = \"log/access.log\" errorlog = \"log/debug.log\" daemon = True #意味着开启后台运行，默认为False workers = 4 # 启动的进程数 threads = 2 #指定每个进程开启的线程数 worker_class = 'gevent' #默认为sync模式，也可使用gevent模式。 x_forwarded_for_header = 'X-FORWARDED-FOR' 启动命令如下 gunicorn -c gunicorn_config.py run:app 三、补充1、关于线程的补充 在工作中我还遇到一种情况，当一个请求过来后，我需要两种回应，一个是及时返回app运行结果，第二个响应是保存数据到日志或者数据库。往往我们在写数据的过程中会花销一定的时间，导致结果返回会有所延迟，因此我们需要用两个线程处理这两个任务，那么我们如下处理。 run.py #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2018-12-01 17:20 # @Author : mokundong from flask import Flask,request from time import sleep from concurrent.futures import ThreadPoolExecutor executor = ThreadPoolExecutor(2) app = Flask(__name__) @app.route('/job') def run_jobs(): executor.submit(some_long_task1) executor.submit(some_long_task2, 'hello', 123) return 'Two jobs was launched in background!' def some_long_task1(): print(\"Task #1 started!\") sleep(10) print(\"Task #1 is done!\") def some_long_task2(arg1, arg2): print(\"Task #2 started with args: %s %s!\" % (arg1, arg2)) sleep(5) print(\"Task #2 is done!\") if __name__ == '__main__': app.run() 2、关于获取IP的补充 上述代码中通过获取hostname，然后再通过hostname反查处机器的IP。这个方法是不推荐的。因为很多的机器没有规范这个hostname的设置。另外就是有些服务器会在 /etc/hosts 中添加本机的hostname的地址，这个做法也不是不可以，但是如果设置成了 127.0.0.1，那么获取出来的IP就都是这个地址了。这里给出一种优雅的方式获取IP，利用 UDP 协议来实现的，生成一个UDP包，把自己的 IP 放如到 UDP 协议头中，然后从UDP包中获取本机的IP。 #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2018-12-01 17:30 # @Author : mokundong # 可以封装成函数，方便 Python 的程序调用 import socket def get_host_ip(): try: s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) s.connect(('8.8.8.8', 80)) ip = s.getsockname()[0] finally: s.close() return ip 总结当然推荐使用gunicorn部署多线程，Flask自带的，emmmm，测试玩儿玩儿吧。在写作过程中才发现自己知识漏洞不是一般多，共勉！","categories":[{"name":"python","slug":"python","permalink":"https://mokundong.cn/categories/python/"},{"name":"flask","slug":"python/flask","permalink":"https://mokundong.cn/categories/python/flask/"}],"tags":[],"keywords":[{"name":"python","slug":"python","permalink":"https://mokundong.cn/categories/python/"},{"name":"flask","slug":"python/flask","permalink":"https://mokundong.cn/categories/python/flask/"}]},{"title":"keras-test","slug":"python-keras-test","date":"2018-12-12T14:16:01.000Z","updated":"2019-11-01T01:34:13.188Z","comments":true,"path":"python-keras-test/","link":"","permalink":"https://mokundong.cn/python-keras-test/","excerpt":"","text":"12import tensorflow as tfimport tensorflow.keras as keras","categories":[{"name":"python","slug":"python","permalink":"https://mokundong.cn/categories/python/"},{"name":"keras","slug":"python/keras","permalink":"https://mokundong.cn/categories/python/keras/"}],"tags":[],"keywords":[{"name":"python","slug":"python","permalink":"https://mokundong.cn/categories/python/"},{"name":"keras","slug":"python/keras","permalink":"https://mokundong.cn/categories/python/keras/"}]}]}